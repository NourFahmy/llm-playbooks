{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_DIR = os.path.expanduser(\"~/Data/lichess\")\n",
    "DATASETS = [\n",
    "    \"standard_rated_2013-01_filtered.jsonl.gz\",  # 7.1 MB\n",
    "    # \"standard_rated_2017-02_filtered.jsonl.gz\",  # 682 MB\n",
    "    # \"standard_rated_2019-03_filtered.jsonl.gz\",  # 3.8 GB\n",
    "    # \"standard_rated_2019-10_filtered.jsonl.gz\",  # 4.3 GB\n",
    "]\n",
    "\n",
    "SEED = 1337\n",
    "VAL_FRAC = 0.1\n",
    "CHARS_PER_TOKEN = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37938"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from chesslm.lichess import LichessPGNEntry\n",
    "\n",
    "entries: list[LichessPGNEntry] = []\n",
    "for dataset in DATASETS:\n",
    "    with gzip.open(os.path.join(DATASET_DIR, dataset), \"rb\") as f:\n",
    "        entries += list(map(LichessPGNEntry.model_validate_json, f.readlines()))\n",
    "\n",
    "display(n_entries := len(entries))\n",
    "print((example := entries[0].sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 4, 72, 62, 4, 72, 64, 4, 9, 4, 71, 62, 4, 69, 64, 4, 10, 4, 68, 61, 4, 76, 69, 65]\n",
      "<start>1. e4 e6 2. d4 b6 3. a3 Bb7\n"
     ]
    }
   ],
   "source": [
    "from chesslm.tokenizer import PGNTokenizer\n",
    "\n",
    "encoder = PGNTokenizer()\n",
    "example_tokens = encoder.encode(example)[:24 + 1]\n",
    "\n",
    "print(example_tokens)\n",
    "print(encoder.decode(example_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  5,  4, 72, 62,  4],\n",
      "        [72, 64,  4,  9,  4, 71],\n",
      "        [62,  4, 69, 64,  4, 10],\n",
      "        [ 4, 68, 61,  4, 76, 69]])\n",
      "tensor([[ 5,  4, 72, 62,  4, 72],\n",
      "        [64,  4,  9,  4, 71, 62],\n",
      "        [ 4, 69, 64,  4, 10,  4],\n",
      "        [68, 61,  4, 76, 69, 65]])\n"
     ]
    }
   ],
   "source": [
    "example_buf = torch.tensor(example_tokens)\n",
    "example_x = example_buf[:-1].view(4, 6)\n",
    "example_y = example_buf[1:].view(4, 6)  # predict the next token\n",
    "\n",
    "print(example_x)\n",
    "print(example_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0<end> <start>1. e4 g6 2. d4 d6 3. Nf3 c6 4. h3 \n"
     ]
    }
   ],
   "source": [
    "n_val_entries = int(VAL_FRAC * n_entries)\n",
    "\n",
    "def get_sequences(entries: list[LichessPGNEntry]) -> list[str]:\n",
    "    return [encoder.add_special_tokens(entry.plain_sequence) for entry in entries]\n",
    "\n",
    "train_text = \" \".join(get_sequences(entries[:-n_val_entries]))\n",
    "val_text = \" \".join(get_sequences(entries[-n_val_entries:]))\n",
    "\n",
    "print(train_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from chesslm.tokenizer import UNK_TOKEN_ID\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(\n",
    "        self,\n",
    "        B: int,\n",
    "        T: int,\n",
    "        text: str,\n",
    "        loop: bool = False,\n",
    "    ):\n",
    "        self.B: int = B\n",
    "        self.T: int = T\n",
    "        self.text: str = text\n",
    "        self.loop: bool = loop\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def BT(self) -> int:\n",
    "        return self.B * self.T\n",
    "\n",
    "    def reset(self):\n",
    "        self._buf = []\n",
    "        self._pos = 0\n",
    "\n",
    "    def next_tokens(self, add_pred_token: bool = False) -> list[int]:\n",
    "        BT = self.BT\n",
    "        pos_step = int((BT + 1) * CHARS_PER_TOKEN)\n",
    "\n",
    "        while len(self._buf) < BT + 1:\n",
    "            segment = self.text[self._pos : self._pos + pos_step]\n",
    "            if not(segment):\n",
    "                raise RuntimeError(\"no tokens remaining\")\n",
    "\n",
    "            # e.g. \"6. Be2 O-O 7. O-\"\n",
    "            if match := re.search(r\" +\", segment[::-1]):\n",
    "                segment = segment[: len(segment) - match.end() + 1]\n",
    "            tokens = encoder.encode(segment, add_special_tokens=False)\n",
    "\n",
    "            if UNK_TOKEN_ID in tokens:\n",
    "                unk_token_idx = len(\n",
    "                    encoder.decode(tokens[: tokens.index(UNK_TOKEN_ID) - 1])\n",
    "                )\n",
    "                unk_token_text = segment[unk_token_idx - 50 : unk_token_idx]\n",
    "                unk_token_text += \" <unk> \"\n",
    "                unk_token_text += segment[unk_token_idx : unk_token_idx + 50]\n",
    "                raise ValueError(f\"<unk> found: '{unk_token_text}'\")\n",
    "\n",
    "            self._buf.extend(tokens)\n",
    "            self._pos += len(segment)\n",
    "\n",
    "        tokens = self._buf[: BT + add_pred_token]  # we want this many tokens\n",
    "        self._buf = self._buf[BT :]  # remove BT tokens (not BT + 1!)\n",
    "        return tokens\n",
    "\n",
    "    def next_batch(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_tokens = torch.as_tensor(self.next_tokens(add_pred_token=True))\n",
    "        try:\n",
    "            x = (batch_tokens[:-1]).view(self.B, self.T) # inputs\n",
    "            y = (batch_tokens[1:]).view(self.B, self.T) # targets\n",
    "        except RuntimeError:\n",
    "            raise RuntimeError(\"no more batches remaining\")\n",
    "        return x, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        try:\n",
    "            return self.next_batch()\n",
    "        except RuntimeError:\n",
    "            if self.loop:\n",
    "                self.reset()\n",
    "                try:\n",
    "                    return self.next_batch()\n",
    "                except RuntimeError:\n",
    "                    raise StopIteration\n",
    "            else:\n",
    "                raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 4, 72, 62, 4, 72, 64, 4, 9, 4, 71, 62, 4, 69, 64, 4, 10, 4]\n",
      "[4, 68, 61, 4, 76, 69, 65, 4, 11, 4, 78, 70, 61, 4, 78, 75, 64, 4, 12]\n",
      "[12, 4, 76, 84, 75, 64, 4, 74, 84, 75, 64, 4, 13, 4, 76, 72, 60, 4, 79]\n",
      "[79, 74, 63, 4, 14, 4, 76, 74, 62, 4, 75, 63, 4, 15, 4, 78, 73, 61, 4]\n",
      "\n",
      "<start>1. e4 e6 2. d4 b6 3. \n",
      " a3 Bb7 4. Nc3 Nh6 5.\n",
      "5. Bxh6 gxh6 6. Be2 Q\n",
      "Qg5 7. Bg4 h5 8. Nf3 \n",
      "\n",
      "[0, 5, 4, 72, 62, 4, 72, 64, 4, 9, 4, 71, 62, 4, 69, 64, 4, 10]\n",
      "[4, 68, 61, 4, 76, 69, 65, 4, 11, 4, 78, 70, 61, 4, 78, 75, 64, 4]\n",
      "[12, 4, 76, 84, 75, 64, 4, 74, 84, 75, 64, 4, 13, 4, 76, 72, 60, 4]\n",
      "[79, 74, 63, 4, 14, 4, 76, 74, 62, 4, 75, 63, 4, 15, 4, 78, 73, 61]\n",
      "\n",
      "<start>1. e4 e6 2. d4 b6 3.\n",
      " a3 Bb7 4. Nc3 Nh6 \n",
      "5. Bxh6 gxh6 6. Be2 \n",
      "Qg5 7. Bg4 h5 8. Nf3\n",
      "\n",
      "<start>1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0<end> <start>1. e4 g6 2. d4 d6 3. Nf3 c6 4. h3 \n"
     ]
    }
   ],
   "source": [
    "example = train_text[:200]\n",
    "dataloader = DataLoaderLite(3, 6, example)\n",
    "\n",
    "def print_next_tokens(add_pred_token: bool):\n",
    "    print(dataloader.next_tokens(add_pred_token=add_pred_token))\n",
    "\n",
    "def print_next_segment(add_pred_token: bool):\n",
    "    print(encoder.decode(dataloader.next_tokens(add_pred_token=add_pred_token)).replace(\"\\n\", \"\\\\n\"))\n",
    "\n",
    "dataloader.reset()\n",
    "print_next_tokens(True)\n",
    "print_next_tokens(True)\n",
    "print_next_tokens(True)\n",
    "print_next_tokens(True)\n",
    "print()\n",
    "dataloader.reset()\n",
    "print_next_segment(True)\n",
    "print_next_segment(True)\n",
    "print_next_segment(True)\n",
    "print_next_segment(True)\n",
    "print()\n",
    "dataloader.reset()\n",
    "print_next_tokens(False)\n",
    "print_next_tokens(False)\n",
    "print_next_tokens(False)\n",
    "print_next_tokens(False)\n",
    "print()\n",
    "dataloader.reset()\n",
    "print_next_segment(False)\n",
    "print_next_segment(False)\n",
    "print_next_segment(False)\n",
    "print_next_segment(False)\n",
    "\n",
    "print()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CosineLRSchedule(BaseModel):\n",
    "    max_lr: float\n",
    "    min_lr: float\n",
    "    warmup_steps: float\n",
    "    max_steps: float\n",
    "\n",
    "    def __call__(self, it: int) -> float:\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.warmup_steps:\n",
    "            return self.max_lr * (it + 1) / self.warmup_steps\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > self.max_steps:\n",
    "            return self.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1 and goes to 0\n",
    "        return self.min_lr + coeff * (self.max_lr - self.min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chesslm.gpt import GPT\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def evaluate_model(model: GPT, dataloader: DataLoaderLite, val_steps: int = 20) -> float:\n",
    "    model.eval()\n",
    "    dataloader.reset()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for _ in range(val_steps):\n",
    "            x, y = dataloader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                _, loss = model(x, y)\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().item()\n",
    "    return val_loss\n",
    "\n",
    "def write_model_checkpoint(\n",
    "        model: GPT,\n",
    "        step: int,\n",
    "        val_loss: float,\n",
    "        fpath: str\n",
    "    ):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'config': model.config,\n",
    "        'step': step,\n",
    "        'val_loss': val_loss\n",
    "    }\n",
    "    # you might also want to add optimizer.state_dict() and\n",
    "    # rng seeds etc., if you wanted to more exactly resume training\n",
    "    torch.save(checkpoint, fpath)\n",
    "\n",
    "def generate_sequences(model: GPT, prefix: str, num_sequences: int, max_length: int) -> list[str]:\n",
    "    tokens = encoder.encode(prefix)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_sequences, 1)\n",
    "    xgen = tokens.to(device)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(SEED)\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, _ = model(xgen) # (B, T, vocab_size)\n",
    "\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "    return [encoder.decode(x[:max_length].tolist()) for x in xgen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15736652\n",
      "245879\n"
     ]
    }
   ],
   "source": [
    "from chesslm.gpt import GPTConfig\n",
    "\n",
    "MAX_STEPS = 100\n",
    "WARMUP_STEPS = 10\n",
    "\n",
    "PRINT_TRAIN_INTERVAL = 5\n",
    "EVAL_INTERVAL = 25\n",
    "EVAL_NUM_SEQUENCES = 4\n",
    "EVAL_SEQUENCE_MAX_LENGTH = 32\n",
    "\n",
    "MAX_LR = 6e-4\n",
    "MIN_LR = MAX_LR * 0.1\n",
    "WEIGHT_DECAY = 0.1\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "\n",
    "T = 512\n",
    "GPT_CONFIG = GPTConfig(\n",
    "    block_size=T,\n",
    "    vocab_size=encoder.n_vocab,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=384,\n",
    ")\n",
    "\n",
    "B = 4\n",
    "BT = B * T\n",
    "\n",
    "EVAL_SEQUENCE = \" \".join(val_text.split(\" \")[:10])\n",
    "\n",
    "print(len(train_text))\n",
    "print(int((BT + 1) * CHARS_PER_TOKEN * MAX_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 18, with 7,307,904 parameters\n",
      "num non-decayed parameter tensors: 34, with 20,736 parameters\n",
      "using fused AdamW: False\n",
      "\n",
      "--------\n",
      "Validation loss: 4.5987\n",
      "Sample 1: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end>31.31.\n",
      "Sample 2: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end>34.7.\n",
      "Sample 3: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end>R16.\n",
      "Sample 4: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end>49.R\n",
      "--------\n",
      "\n",
      "step     0 | loss: 4.603565 | lr 6.0000e-05 | norm: 19.5527 | dt: 1357.80ms | tok/sec: 3016.65\n",
      "step     5 | loss: 3.266981 | lr 3.6000e-04 | norm: 3.1363 | dt: 149.89ms | tok/sec: 27327.06\n",
      "step    10 | loss: 2.757719 | lr 6.0000e-04 | norm: 3.6970 | dt: 141.78ms | tok/sec: 28890.82\n",
      "step    15 | loss: 2.229734 | lr 5.9590e-04 | norm: 1.2082 | dt: 141.34ms | tok/sec: 28979.17\n",
      "step    20 | loss: 2.128807 | lr 5.8372e-04 | norm: 0.8028 | dt: 141.26ms | tok/sec: 28996.78\n",
      "\n",
      "--------\n",
      "Validation loss: 2.0854\n",
      "Sample 1: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> N\n",
      "Sample 2: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> B\n",
      "Sample 3: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> N\n",
      "Sample 4: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> Q\n",
      "--------\n",
      "\n",
      "step    25 | loss: 2.081012 | lr 5.6383e-04 | norm: 0.8110 | dt: 607.23ms | tok/sec: 6745.42\n",
      "step    30 | loss: 2.095447 | lr 5.3683e-04 | norm: 0.7384 | dt: 140.25ms | tok/sec: 29205.25\n",
      "step    35 | loss: 2.048074 | lr 5.0355e-04 | norm: 0.6566 | dt: 141.54ms | tok/sec: 28939.83\n",
      "step    40 | loss: 2.037258 | lr 4.6500e-04 | norm: 1.2857 | dt: 141.03ms | tok/sec: 29043.25\n",
      "step    45 | loss: 2.009919 | lr 4.2235e-04 | norm: 1.0058 | dt: 141.31ms | tok/sec: 28986.80\n",
      "\n",
      "--------\n",
      "Validation loss: 1.9833\n",
      "Sample 1: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> N\n",
      "Sample 2: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> d\n",
      "Sample 3: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> N\n",
      "Sample 4: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> B\n",
      "--------\n",
      "\n",
      "step    50 | loss: 1.990455 | lr 3.7689e-04 | norm: 0.6587 | dt: 605.81ms | tok/sec: 6761.17\n",
      "step    55 | loss: 1.968056 | lr 3.3000e-04 | norm: 0.7363 | dt: 143.93ms | tok/sec: 28458.66\n",
      "step    60 | loss: 1.947565 | lr 2.8311e-04 | norm: 0.5480 | dt: 144.33ms | tok/sec: 28378.79\n",
      "step    65 | loss: 1.910762 | lr 2.3765e-04 | norm: 0.5981 | dt: 141.61ms | tok/sec: 28924.53\n",
      "step    70 | loss: 1.944105 | lr 1.9500e-04 | norm: 0.6745 | dt: 146.92ms | tok/sec: 27878.31\n",
      "\n",
      "--------\n",
      "Validation loss: 1.8976\n",
      "Sample 1: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> B\n",
      "Sample 2: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> e\n",
      "Sample 3: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> B\n",
      "Sample 4: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> d\n",
      "--------\n",
      "\n",
      "step    75 | loss: 1.886942 | lr 1.5645e-04 | norm: 0.5629 | dt: 606.01ms | tok/sec: 6758.96\n",
      "step    80 | loss: 1.909081 | lr 1.2317e-04 | norm: 0.7884 | dt: 142.34ms | tok/sec: 28775.80\n",
      "step    85 | loss: 1.863693 | lr 9.6173e-05 | norm: 0.6236 | dt: 146.24ms | tok/sec: 28008.94\n",
      "step    90 | loss: 1.864367 | lr 7.6283e-05 | norm: 0.6336 | dt: 141.54ms | tok/sec: 28939.83\n",
      "step    95 | loss: 1.853744 | lr 6.4102e-05 | norm: 0.5962 | dt: 141.30ms | tok/sec: 28988.17\n",
      "\n",
      "--------\n",
      "Validation loss: 1.8613\n",
      "Sample 1: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> B\n",
      "Sample 2: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> e\n",
      "Sample 3: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> B\n",
      "Sample 4: <start>1. e4 f6 2. Nc3 e5 3. Qh5+ g6 4.<end> d\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = GPT(GPT_CONFIG).to(device)\n",
    "\n",
    "cosine_schedule = CosineLRSchedule(\n",
    "    max_lr=MAX_LR,\n",
    "    min_lr=MIN_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, text=train_text)\n",
    "val_loader = DataLoaderLite(B=B, T=T, text=val_text, loop=True)\n",
    "\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=MAX_LR,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "for step in range(MAX_STEPS):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == MAX_STEPS - 1)\n",
    "\n",
    "    # once in a while:\n",
    "    # evaluate our validation loss\n",
    "    # generate from the model and print\n",
    "    if step % EVAL_INTERVAL == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = evaluate_model(model, val_loader)\n",
    "            sequences = generate_sequences(\n",
    "                model,\n",
    "                EVAL_SEQUENCE,\n",
    "                EVAL_NUM_SEQUENCES,\n",
    "                EVAL_SEQUENCE_MAX_LENGTH,\n",
    "            )\n",
    "\n",
    "        print(\"\\n--------\")\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "        for i, seq in enumerate(sequences, 1):\n",
    "            seq = seq.replace('\\n', '\\\\n')\n",
    "            print(f\"Sample {i}: {seq}\")\n",
    "        print(\"--------\\n\")\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "    for micro_step in range(GRAD_ACCUM_STEPS):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        loss = loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.detach().item()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr = cosine_schedule(step)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = BT * GRAD_ACCUM_STEPS\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    if step % PRINT_TRAIN_INTERVAL == 0:\n",
    "        print(\n",
    "            f\"step {step:5d}\"\n",
    "            f\" | loss: {train_loss:.6f}\"\n",
    "            f\" | lr {lr:.4e}\"\n",
    "            f\" | norm: {norm:.4f}\"\n",
    "            f\" | dt: {dt*1000:.2f}ms\"\n",
    "            f\" | tok/sec: {tokens_per_sec:.2f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
